# -*- coding: utf-8 -*-
"""DLtask-handdigitrecog.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NgLAh51Hv2dpWiudN7elGbG-G8Z1NTsb

importing some of the needed libraries and functions


---
"""

from torchvision import datasets
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

"""downloads train data and splits to train and test data


---


"""

train_data = datasets.MNIST(  #sets parameter for picking train data
    root = 'data',
    train = True,
    transform= ToTensor(),
    download = True
)
test_data = datasets.MNIST(   #sets parameter for picking train data
    root = 'data',
    train =False,
    transform= ToTensor(),
    download = True
)

"""show info about the data to make sure

---


"""

train_data

test_data

train_data.targets  #shows the digits that are supposed to be
                    #the correct target for each tensor transformed image

"""Make the dataloaders to allow inputing the data in batches

---




"""

Loaders = {
     'train' : DataLoader(train_data,batch_size=100,shuffle=True,num_workers=1),
     'test' : DataLoader(test_data,batch_size=100,shuffle=True,num_workers=1)
}

"""Define DL architechure and model

---


"""

class CNN(nn.Module):
    def __init__(self):
      super(CNN,self).__init__()

      self.conv1 = nn.Conv2d(1,10,kernel_size=5)#defines first layer with 1 (going to be image tensor) to 10 connected convolutionaly
      self.conv2 = nn.Conv2d(10,20,kernel_size=5) #defines another layer connected to first each 1 connected to 2 convolutionally
      self.conv2_drop = nn.Dropout2d()         #defines that the second layer will have node drop out
      self.fc1 = nn.Linear(320,50)             # defines the before the last layer connected linearly
      self.fc2 = nn.Linear(50,10)              # defines that last layer connected linearly

    def forward(self,x):            #defines the forwarding mechanism from start to last layer with each having its own activation function
      x = F.relu(F.max_pool2d(self.conv1(x),2))
      x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))
      x = x.view(-1,320)
      x = F.relu(self.fc1(x))
      x = F.dropout(x, training=self.training)
      x = self.fc2(x)

      return F.softmax(x)

"""Define training and testing processes

---


"""

import torch

#define the device to work on , on GPU if the option is avalaiable cause it's faster
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#move the model  also the same device
model=CNN().to(device)

#define the optimization used
optimizer = optim.Adam(model.parameters(),lr=0.0015)

#define the loss function that will be used
loss_fn = nn.CrossEntropyLoss()

#define the training methodology
def train(epoch):
  model.train()      #put model in training mode
  for batch_idx,(data, target) in enumerate(Loaders['train']):  #loop over data in the batches
    data,target = data.to(device), target.to(device)            #make sure both data and target are on same device
    optimizer.zero_grad()
    output = model(data)      #get the supposed estimation of the model on the data
    loss = loss_fn(output,target)   #calculate loss using chosen loss function
    loss.backward()                 #backward propagate the loss
    optimizer.step()                # take a step of the chosen learning rate towards the optimum
    if batch_idx % 25 == 0:                                     #extra thing just to print as it processes how far its on and the loss and so
      print(f'train Epoch:{epoch} [{batch_idx * len(data)}/{len(Loaders["train"].dataset)} ({100.*batch_idx / len(Loaders["train"]):.0f}%)]\t{loss.item():.6f}')


#define the test methodology
def test():
  model.eval()  #puts model in test mode

  test_loss = 0     #initialize the loss and amount of correct to 0
  correct = 0

  with torch.no_grad():
    for data,target in Loaders['test']:       #loop over the samples from the test batch loader
      data,target = data.to(device),target.to(device)
      output = model(data)
      test_loss += loss_fn(output,target).item()
      pred = output.argmax(dim=1,keepdim=True)
      correct += pred.eq(target.view_as(pred)).sum().item()

  test_loss /= len(Loaders['test'].dataset)
  print(f'\n Test set : Average loss: {test_loss:.4f}, Accuracy {correct}/{len(Loaders["test"].dataset)} ({100.*correct / len(Loaders["test"].dataset):.0f}%\n)')

"""Start the actual training of the model
and track the accuracy and test set

---


"""

for epoch in range (1,11):  #loop and train 10 epoch and test after each epoch
  train(epoch)
  test()

"""sadly i dont have GPU option :(

---


"""

device

"""visualize some of the samples"""

import matplotlib.pyplot as plt

model.eval()

for sample_idx in range (0,3):
  data,target = test_data[sample_idx]
  data = data.unsqueeze(0).to(device)
  output =  model(data)
  prediction = output.argmax(dim = 1,keepdim=True).item()

  print(f'prediction : {prediction}')

  image =  data.squeeze(0).squeeze(0).cpu().numpy()

  plt.imshow(image,cmap='gray')
  plt.show()